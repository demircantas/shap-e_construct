{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ccced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
    "from shap_e.util.image_util import load_image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eed3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1570073",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d922637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xm is for rendering latents\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('image300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a154c6",
   "metadata": {},
   "source": [
    "# compute latents from image\n",
    "- [-] As of March 25 2024, this cell is running very slow (19 minutes and still going for a single batch). I remember this step taking far less. Is this a problem with hyperparameters or a temporary issue with the kernel?\n",
    "    - [-] The hyperparameters are at default values.\n",
    "    - [-] Rendering interpolation takes a lot longer than rendering for a single object\n",
    "## computing time\n",
    "| batch size | time |\n",
    "| --- | --- |\n",
    "| 1 | 3m, 26s |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d329d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234b0b0d603141a995dd1ba1394cf84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../content/cube_bbg.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_latent:\n\u001b[0;32m---> 12\u001b[0m     latents \u001b[38;5;241m=\u001b[39m \u001b[43msample_latents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_karras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkarras_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigma_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_churn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/sample.py:62\u001b[0m, in \u001b[0;36msample_latents\u001b[0;34m(batch_size, model, diffusion, model_kwargs, guidance_scale, clip_denoised, use_fp16, use_karras, karras_steps, sigma_min, sigma_max, s_churn, device, progress)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39muse_fp16):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_karras:\n\u001b[0;32m---> 62\u001b[0m         samples \u001b[38;5;241m=\u001b[39m \u001b[43mkarras_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkarras_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43msigma_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43ms_churn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_churn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         internal_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/k_diffusion.py:113\u001b[0m, in \u001b[0;36mkarras_sample\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkarras_sample\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    112\u001b[0m     last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m karras_sample_progressive(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m         last \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/k_diffusion.py:181\u001b[0m, in \u001b[0;36mkarras_sample_progressive\u001b[0;34m(diffusion, model, shape, steps, clip_denoised, progress, model_kwargs, device, sigma_min, sigma_max, rho, sampler, s_churn, s_tmin, s_tmax, s_noise, guidance_scale)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     guided_denoiser \u001b[38;5;241m=\u001b[39m denoiser\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m sample_fn(\n\u001b[1;32m    182\u001b[0m     guided_denoiser,\n\u001b[1;32m    183\u001b[0m     x_T,\n\u001b[1;32m    184\u001b[0m     sigmas,\n\u001b[1;32m    185\u001b[0m     progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msampler_args,\n\u001b[1;32m    187\u001b[0m ):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(diffusion, GaussianDiffusion):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m diffusion\u001b[38;5;241m.\u001b[39munscale_out_dict(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/shap-e/lib/python3.9/site-packages/torch/utils/_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 56\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/k_diffusion.py:275\u001b[0m, in \u001b[0;36msample_heun\u001b[0;34m(denoiser, x, sigmas, progress, s_churn, s_tmin, s_tmax, s_noise)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Heun's method\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     x_2 \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m d \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m--> 275\u001b[0m     denoised_2 \u001b[38;5;241m=\u001b[39m \u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     d_2 \u001b[38;5;241m=\u001b[39m to_d(x_2, sigmas[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], denoised_2)\n\u001b[1;32m    277\u001b[0m     d_prime \u001b[38;5;241m=\u001b[39m (d \u001b[38;5;241m+\u001b[39m d_2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/k_diffusion.py:173\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.guided_denoiser\u001b[0;34m(x_t, sigma)\u001b[0m\n\u001b[1;32m    171\u001b[0m x_t \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([x_t, x_t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    172\u001b[0m sigma \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([sigma, sigma], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m \u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m cond_x_0, uncond_x_0 \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msplit(x_0, \u001b[38;5;28mlen\u001b[39m(x_0) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    175\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m uncond_x_0 \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (cond_x_0 \u001b[38;5;241m-\u001b[39m uncond_x_0)\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/k_diffusion.py:160\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.denoiser\u001b[0;34m(x_t, sigma)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoiser\u001b[39m(x_t, sigma):\n\u001b[0;32m--> 160\u001b[0m     _, denoised \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m denoised\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/k_diffusion.py:105\u001b[0m, in \u001b[0;36mGaussianToKarrasDenoiser.denoise\u001b[0;34m(self, x_t, sigmas, clip_denoised, model_kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m t \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    100\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_to_t(sigma) \u001b[38;5;28;01mfor\u001b[39;00m sigma \u001b[38;5;129;01min\u001b[39;00m sigmas\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()],\n\u001b[1;32m    101\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mlong,\n\u001b[1;32m    102\u001b[0m     device\u001b[38;5;241m=\u001b[39msigmas\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m c_in \u001b[38;5;241m=\u001b[39m append_dims(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (sigmas\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, x_t\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m--> 105\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_xstart\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/gaussian_diffusion.py:346\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     model_variance \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mexp(model_log_variance)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     min_log \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_into_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_log_variance_clipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     max_log \u001b[38;5;241m=\u001b[39m _extract_into_tensor(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbetas), t, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# The model_var_values is [-1, 1] for [min_var, max_var].\u001b[39;00m\n",
      "File \u001b[0;32m~/sm-thesis/shap-e_construct/shap_e/diffusion/gaussian_diffusion.py:1068\u001b[0m, in \u001b[0;36m_extract_into_tensor\u001b[0;34m(arr, timesteps, broadcast_shape)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_into_tensor\u001b[39m(arr, timesteps, broadcast_shape):\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    Extract values from a 1-D numpy array for a batch of indices.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[timesteps]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(broadcast_shape):\n\u001b[1;32m   1070\u001b[0m         res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_latent = False\n",
    "batch_size = 1\n",
    "guidance_scale = 3.0\n",
    "\n",
    "# To get the best result, you should remove the background and show only the object of interest to the model.\n",
    "# image = load_image(\"example_data/corgi.png\")\n",
    "# image = load_image(\"example_data/cube.png\")\n",
    "image = load_image(\"../../../content/cube_tall_bbg.png\")\n",
    "image = load_image(\"../../../content/cube_bbg.png\")\n",
    "\n",
    "if create_latent:\n",
    "    latents = sample_latents(\n",
    "        batch_size=batch_size,\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        guidance_scale=guidance_scale,\n",
    "        model_kwargs=dict(images=[image] * batch_size),\n",
    "        progress=True,\n",
    "        clip_denoised=True,\n",
    "        use_fp16=True,\n",
    "        use_karras=True,\n",
    "        karras_steps=64,\n",
    "        sigma_min=1e-3,\n",
    "        sigma_max=160,\n",
    "        s_churn=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02fbab",
   "metadata": {},
   "source": [
    "# latent loading, manipulation, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load latents from file\n",
    "if 0:\n",
    "    latents_cube = torch.load('../../latents/cube_latents.pt')\n",
    "    latents_cube_tall = torch.load('../../latents/cube_tall_latents.pt')\n",
    "    # print minimum and maximum values of latents\n",
    "    print(f'latents_cube min: {latents_cube.min()}')\n",
    "    print(f'latents_cube max: {latents_cube.max()}')\n",
    "    print(f'latents_cube_tall min: {latents_cube_tall.min()}')\n",
    "    print(f'latents_cube_tall max: {latents_cube_tall.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function for plotting a histogram of latents\n",
    "def plot_hist(latents):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(latents.flatten().cpu().numpy(), bins=500)\n",
    "    plt.show()\n",
    "\n",
    "# function for interpolating among two latent vectors\n",
    "def interpolate_latents(lat_A, lat_B, intp_steps):\n",
    "    latents = []\n",
    "    for i in range(intp_steps):\n",
    "        latents.append(lat_A + (lat_B - lat_A) * i / intp_steps)\n",
    "    return latents\n",
    "\n",
    "# function for extrapolating from a latent vector\n",
    "def extrapolate_latents(lat_A, lat_B, extp_steps):\n",
    "    latents = []\n",
    "    for i in range(extp_steps):\n",
    "        latents.append(lat_B + (lat_B - lat_A) * i / extp_steps)\n",
    "    return latents\n",
    "\n",
    "# function for extracting transformation from two latent vectors\n",
    "def extract_transformation(lat_A, lat_B):\n",
    "    return lat_B - lat_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_transform = extract_transformation(latents_cube[0], latents_cube_tall[0])\n",
    "# print the indices of five largest values in the latent_transform vector\n",
    "print(torch.topk(latent_transform, 5))\n",
    "\n",
    "plot_hist(latents_cube)\n",
    "plot_hist(latent_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate between two latents\n",
    "intp_latents = interpolate_latents(latents_cube[3], latents_cube_tall[0], 20)\n",
    "extp_latents = extrapolate_latents(latents_cube_tall[0], latents_cube[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2a887",
   "metadata": {},
   "source": [
    "# render latents\n",
    "\n",
    "## *render_transformation* function below took:\n",
    "\n",
    "| size | samples | time |\n",
    "| --- | --- | --- |\n",
    "| 32 | 5 | 15m21.0s |\n",
    "| 32 | 10 | 38m25.5s |\n",
    "| 32 | 10 | 54m50.5s |\n",
    "|___|___|___|\n",
    "| 64 | 20 | 14m5.7s |\n",
    "| 64 | 20 | 19m40.4s |\n",
    "| 64 | 20 | 31m.52.9s |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f14a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'nerf' # you can change this to 'stf' for mesh rendering\n",
    "# size = 64 # this is the size of the renders; higher values take longer to render.\n",
    "size = 32 # this is the size of the renders; higher values take longer to render. 16 causes an assertion error.\n",
    "cameras = create_pan_cameras(size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6f5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rendering interpolated or extrapolated latents\n",
    "def render_latents(latents):\n",
    "    for i, latent in enumerate(latents):\n",
    "        images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    return images\n",
    "    # images = []\n",
    "    # for latent in latents:\n",
    "    #     images.append(decode_latent_images(xm, latent, cameras, rendering_mode=render_mode))\n",
    "    #     # images[0].save('../../../content/{}_{}.gif'.format(name, i), save_all=True, append_images=images[1:], duration=100, loop=0)\n",
    "    # return images\n",
    "\n",
    "# function for rendering the interpolation between two latents as a single gif\n",
    "def render_transformation(latents, name):\n",
    "    # render the first frame from the first latent, second frame from the second latent, and so on\n",
    "    images = []\n",
    "    for i, latent in enumerate(latents):\n",
    "        images.append(decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)[0])\n",
    "        \n",
    "    # write images to a gif\n",
    "    images[0].save('../../../content/{}.gif'.format(name), save_all=True, append_images=images[1:], duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a029946",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    render_transformation(intp_latents[0:20], 'transformation_render_test_20_64_3')\n",
    "\n",
    "if 0:\n",
    "    # render_latents(extp_latents, size, 'cube_intp')\n",
    "    images = render_latents(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89587cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[0].save('../../../content/{}_{}.gif'.format('cube_debug', '0'), save_all=True, append_images=images[1:], duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd568",
   "metadata": {},
   "source": [
    "- 17 minutes for size = 128\n",
    "- 12 minutes for size = 64\n",
    "- 50 seconds for size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb717d9",
   "metadata": {},
   "source": [
    "# save latents to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save latents to file\n",
    "if 0:\n",
    "    torch.save(latents, '../../latents/cube_latents_bbg.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bac3ab",
   "metadata": {},
   "source": [
    "# Automation for folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45809ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cube_tall_bbg.png', 'cube_bbg.png']\n",
      "../../../content/batch_sourceimages/cube_tall_bbg.png\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"../../../content/batch_sourceimages\"\n",
    "output_folder = \"../../../content/batch_output\"\n",
    "\n",
    "image_files = [file for file in os.listdir(input_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "print(image_files)\n",
    "image_path = os.path.join(input_folder, image_files[0])\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "714713d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(input_folder, output_folder):\n",
    "    # Get a list of image files in the input folder\n",
    "    image_files = [file for file in os.listdir(input_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Load the image\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        image = load_image(image_path)\n",
    "\n",
    "        batch_size = 1\n",
    "        guidance_scale = 3.0\n",
    "\n",
    "        # computing latent\n",
    "        latent_vector = sample_latents(\n",
    "            batch_size=batch_size,\n",
    "            model=model,\n",
    "            diffusion=diffusion,\n",
    "            guidance_scale=guidance_scale,\n",
    "            model_kwargs=dict(images=[image] * batch_size),\n",
    "            progress=True,\n",
    "            clip_denoised=True,\n",
    "            use_fp16=True,\n",
    "            use_karras=True,\n",
    "            karras_steps=64,\n",
    "            sigma_min=1e-3,\n",
    "            sigma_max=160,\n",
    "            s_churn=0,\n",
    "        )\n",
    "\n",
    "        gif = render_latents(latent_vector)\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        # Save latent vector\n",
    "        latent_vector_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_{index}.npy\")\n",
    "\n",
    "        # Check if a file with same name and index exists and increment index and file name if it does\n",
    "        while os.path.exists(latent_vector_path):\n",
    "            index += 1\n",
    "            print(f\"File {latent_vector_path} already exists, incrementing index to {index}\")\n",
    "            latent_vector_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_{index}.npy\")\n",
    "        torch.save(latent_vector, latent_vector_path)\n",
    "        print(f\"Saved latent vector to {latent_vector_path}\")\n",
    "        del(latent_vector)\n",
    "\n",
    "        # Save GIF\n",
    "        gif_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_{index}.gif\")\n",
    "        gif[0].save(gif_path, save_all=True, append_images=gif[1:], duration=100, loop=0)\n",
    "        print(f\"Saved gif to {gif_path}\")\n",
    "        del(gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16022828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e95d3370dae48cbb08b8c6b8d730b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent vector to ../../../content/batch_output/cube_tall_bbg_0.npy\n",
      "Saved gif to ../../../content/batch_output/cube_tall_bbg_0.gif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b965fe04494ce5b179948799e58dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent vector to ../../../content/batch_output/cube_bbg_0.npy\n",
      "Saved gif to ../../../content/batch_output/cube_bbg_0.gif\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# input_folder = \"../../../content/batch_sourceimages\"\n",
    "input_folder = \"../../../content/batch_web\"\n",
    "output_folder = \"../../../content/batch_output\"\n",
    "process_images(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84c467",
   "metadata": {},
   "source": [
    "- 9m 13.0s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
