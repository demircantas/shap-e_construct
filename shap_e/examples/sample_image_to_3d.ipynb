{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ccced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
    "from shap_e.util.image_util import load_image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eed3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1570073",
   "metadata": {},
   "source": [
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d922637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xm is for rendering latents\n",
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('image300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a154c6",
   "metadata": {},
   "source": [
    "# compute latents from image\n",
    "- [-] As of March 25 2024, this cell is running very slow (19 minutes and still going for a single batch). I remember this step taking far less. Is this a problem with hyperparameters or a temporary issue with the kernel?\n",
    "    - [-] The hyperparameters are at default values.\n",
    "    - [-] Rendering interpolation takes a lot longer than rendering for a single object\n",
    "## computing time\n",
    "| batch size | time |\n",
    "| --- | --- |\n",
    "| 1 | 3m, 26s |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53d329d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9293c9f7ef4ab29ab3588dc765a608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_latent = True\n",
    "batch_size = 1\n",
    "guidance_scale = 3.0\n",
    "\n",
    "# To get the best result, you should remove the background and show only the object of interest to the model.\n",
    "# image = load_image(\"example_data/corgi.png\")\n",
    "# image = load_image(\"example_data/cube.png\")\n",
    "image = load_image(\"../../../content/cube_tall_bbg.png\")\n",
    "image = load_image(\"../../../content/cube_bbg.png\")\n",
    "\n",
    "if create_latent:\n",
    "    latents = sample_latents(\n",
    "        batch_size=batch_size,\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        guidance_scale=guidance_scale,\n",
    "        model_kwargs=dict(images=[image] * batch_size),\n",
    "        progress=True,\n",
    "        clip_denoised=True,\n",
    "        use_fp16=True,\n",
    "        use_karras=True,\n",
    "        karras_steps=64,\n",
    "        sigma_min=1e-3,\n",
    "        sigma_max=160,\n",
    "        s_churn=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02fbab",
   "metadata": {},
   "source": [
    "# latent loading, manipulation, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408b601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load latents from file\n",
    "if 0:\n",
    "    latents_cube = torch.load('../../latents/cube_latents.pt')\n",
    "    latents_cube_tall = torch.load('../../latents/cube_tall_latents.pt')\n",
    "    # print minimum and maximum values of latents\n",
    "    print(f'latents_cube min: {latents_cube.min()}')\n",
    "    print(f'latents_cube max: {latents_cube.max()}')\n",
    "    print(f'latents_cube_tall min: {latents_cube_tall.min()}')\n",
    "    print(f'latents_cube_tall max: {latents_cube_tall.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd4b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function for plotting a histogram of latents\n",
    "def plot_hist(latents):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(latents.flatten().cpu().numpy(), bins=500)\n",
    "    plt.show()\n",
    "\n",
    "# function for interpolating among two latent vectors\n",
    "def interpolate_latents(lat_A, lat_B, intp_steps):\n",
    "    latents = []\n",
    "    for i in range(intp_steps):\n",
    "        latents.append(lat_A + (lat_B - lat_A) * i / intp_steps)\n",
    "    return latents\n",
    "\n",
    "# function for extrapolating from a latent vector\n",
    "def extrapolate_latents(lat_A, lat_B, extp_steps):\n",
    "    latents = []\n",
    "    for i in range(extp_steps):\n",
    "        latents.append(lat_B + (lat_B - lat_A) * i / extp_steps)\n",
    "    return latents\n",
    "\n",
    "# function for extracting transformation from two latent vectors\n",
    "def extract_transformation(lat_A, lat_B):\n",
    "    return lat_B - lat_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_transform = extract_transformation(latents_cube[0], latents_cube_tall[0])\n",
    "# print the indices of five largest values in the latent_transform vector\n",
    "print(torch.topk(latent_transform, 5))\n",
    "\n",
    "plot_hist(latents_cube)\n",
    "plot_hist(latent_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate between two latents\n",
    "intp_latents = interpolate_latents(latents_cube[3], latents_cube_tall[0], 20)\n",
    "extp_latents = extrapolate_latents(latents_cube_tall[0], latents_cube[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2a887",
   "metadata": {},
   "source": [
    "# render latents\n",
    "\n",
    "## *render_transformation* function below took:\n",
    "\n",
    "| size | samples | time |\n",
    "| --- | --- | --- |\n",
    "| 32 | 5 | 15m21.0s |\n",
    "| 32 | 10 | 38m25.5s |\n",
    "| 32 | 10 | 54m50.5s |\n",
    "|___|___|___|\n",
    "| 64 | 20 | 14m5.7s |\n",
    "| 64 | 20 | 19m40.4s |\n",
    "| 64 | 20 | 31m.52.9s |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48f14a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'nerf' # you can change this to 'stf' for mesh rendering\n",
    "# size = 64 # this is the size of the renders; higher values take longer to render.\n",
    "size = 32 # this is the size of the renders; higher values take longer to render. 16 causes an assertion error.\n",
    "cameras = create_pan_cameras(size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rendering interpolated or extrapolated latents\n",
    "def render_latents(latents):\n",
    "    for i, latent in enumerate(latents):\n",
    "        images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    return images\n",
    "    # images = []\n",
    "    # for latent in latents:\n",
    "    #     images.append(decode_latent_images(xm, latent, cameras, rendering_mode=render_mode))\n",
    "    #     # images[0].save('../../../content/{}_{}.gif'.format(name, i), save_all=True, append_images=images[1:], duration=100, loop=0)\n",
    "    # return images\n",
    "\n",
    "# function for rendering the interpolation between two latents as a single gif\n",
    "def render_transformation(latents, name):\n",
    "    # render the first frame from the first latent, second frame from the second latent, and so on\n",
    "    images = []\n",
    "    for i, latent in enumerate(latents):\n",
    "        images.append(decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)[0])\n",
    "        \n",
    "    # write images to a gif\n",
    "    images[0].save('../../../content/{}.gif'.format(name), save_all=True, append_images=images[1:], duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a029946",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    render_transformation(intp_latents[0:20], 'transformation_render_test_20_64_3')\n",
    "\n",
    "if 0:\n",
    "    # render_latents(extp_latents, size, 'cube_intp')\n",
    "    images = render_latents(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d89587cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[0].save('../../../content/{}_{}.gif'.format('cube_debug', '0'), save_all=True, append_images=images[1:], duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd568",
   "metadata": {},
   "source": [
    "- 17 minutes for size = 128\n",
    "- 12 minutes for size = 64\n",
    "- 50 seconds for size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb717d9",
   "metadata": {},
   "source": [
    "# save latents to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0259b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save latents to file\n",
    "if 0:\n",
    "    torch.save(latents, '../../latents/cube_latents_bbg.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bac3ab",
   "metadata": {},
   "source": [
    "# Automation for folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45809ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cube_tall_bbg.png', 'cube_bbg.png']\n",
      "../../../content/batch_sourceimages/cube_tall_bbg.png\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"../../../content/batch_sourceimages\"\n",
    "output_folder = \"../../../content/batch_output\"\n",
    "\n",
    "image_files = [file for file in os.listdir(input_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "print(image_files)\n",
    "image_path = os.path.join(input_folder, image_files[0])\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714713d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(input_folder, output_folder):\n",
    "    # Get a list of image files in the input folder\n",
    "    image_files = [file for file in os.listdir(input_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Load the image\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        image = load_image(image_path)\n",
    "\n",
    "        batch_size = 1\n",
    "        guidance_scale = 3.0\n",
    "\n",
    "        # computing latent\n",
    "        latent_vector = sample_latents(\n",
    "            batch_size=batch_size,\n",
    "            model=model,\n",
    "            diffusion=diffusion,\n",
    "            guidance_scale=guidance_scale,\n",
    "            model_kwargs=dict(images=[image] * batch_size),\n",
    "            progress=True,\n",
    "            clip_denoised=True,\n",
    "            use_fp16=True,\n",
    "            use_karras=True,\n",
    "            karras_steps=64,\n",
    "            sigma_min=1e-3,\n",
    "            sigma_max=160,\n",
    "            s_churn=0,\n",
    "        )\n",
    "\n",
    "        gif = render_latents(latent_vector)\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        # Save latent vector\n",
    "        latent_vector_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_{index}.npy\")\n",
    "\n",
    "        # Check if a file with same name and index exists and increment index and file name if it does\n",
    "        while os.path.exists(latent_vector_path):\n",
    "            index += 1\n",
    "            latent_vector_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_{index}.npy\")\n",
    "        torch.save(latent_vector, latent_vector_path)\n",
    "        del(latent_vector)\n",
    "\n",
    "        # Save GIF\n",
    "        gif_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_{index}.gif\")\n",
    "        gif[0].save(gif_path, save_all=True, append_images=images[1:], duration=100, loop=0)\n",
    "        del(gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16022828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03945cc667d41e19b35cb2eb84307bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_folder = \"../../../content/batch_sourceimages\"\n",
    "output_folder = \"../../../content/batch_output\"\n",
    "process_images(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
